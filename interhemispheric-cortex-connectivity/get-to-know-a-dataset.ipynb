{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb623b1d",
   "metadata": {},
   "source": [
    "# Get To Know A Dataset: Interhemispheric Cortex Connectivity Microscopy Dataset\n",
    "\n",
    "*Institut Pasteur â€” AWS Open Data Sponsorship Program*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc8906b",
   "metadata": {},
   "source": [
    "## 1. What is this dataset?\n",
    "\n",
    "This dataset contains high-resolution **ExA-SPIM** (Expansion-Assisted Selective Plane Illumination Microscopy) light-sheet imaging volumes of mouse brains, focused on mapping interhemispheric cortical connectivity. The data captures individual callosal projection neurons and their axonal arbors crossing between the two brain hemispheres at sub-micron effective resolution.\n",
    "\n",
    "**Key characteristics:**\n",
    "- **Modality:** ExA-SPIM light-sheet fluorescence microscopy\n",
    "- **Species:** Mouse (*Mus musculus*)\n",
    "- **Resolution:** ~300 nm lateral, ~800 nm axial (effective, post-expansion)\n",
    "- **Format:** OME-Zarr (multiscale, cloud-optimized)\n",
    "- **Focus:** Interhemispheric (callosal) cortical projection neurons\n",
    "\n",
    "The dataset is produced by the [Institut Pasteur](https://www.pasteur.fr) and hosted through the [AWS Open Data Sponsorship Program](https://aws.amazon.com/opendata/open-data-sponsorship-program/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e14cc",
   "metadata": {},
   "source": [
    "## 2. How can I access the data?\n",
    "\n",
    "The data is stored in a public Amazon S3 bucket. No AWS account is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2716e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available data assets using boto3\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "bucket = '[BUCKET-NAME-TBD]'\n",
    "\n",
    "# List top-level data assets\n",
    "response = s3.list_objects_v2(Bucket=bucket, Delimiter='/')\n",
    "for prefix in response.get('CommonPrefixes', []):\n",
    "    print(prefix['Prefix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9119b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, using the AWS CLI (run in terminal):\n",
    "# aws s3 ls --no-sign-request s3://[BUCKET-NAME-TBD]/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d0f50",
   "metadata": {},
   "source": [
    "## 3. What does the data look like?\n",
    "\n",
    "Each data asset is a directory following [AIND-style naming conventions](https://allenneuraldynamics.github.io/data.html):\n",
    "\n",
    "```\n",
    "exaspim_<subject-id>_<date>_<time>/\n",
    "    exaspim/\n",
    "        image.ome.zarr/       # Multiscale OME-Zarr volume\n",
    "    data_description.json     # Dataset metadata\n",
    "    subject.json              # Animal information\n",
    "    procedures.json           # Experimental procedures\n",
    "    acquisition.json          # Microscope settings\n",
    "    processing.json           # Processing provenance\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e63d490",
   "metadata": {},
   "source": [
    "### Reading metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c483ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# Read subject metadata for an example asset\n",
    "example_asset = 'exaspim_[EXAMPLE-SUBJECT]_[DATE]_[TIME]'\n",
    "\n",
    "with fs.open(f'{bucket}/{example_asset}/subject.json', 'r') as f:\n",
    "    subject = json.load(f)\n",
    "    print(\"=== Subject metadata ===\")\n",
    "    print(json.dumps(subject, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd0eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read acquisition parameters\n",
    "with fs.open(f'{bucket}/{example_asset}/acquisition.json', 'r') as f:\n",
    "    acquisition = json.load(f)\n",
    "    print(\"=== Acquisition parameters ===\")\n",
    "    print(json.dumps(acquisition, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09befcbf",
   "metadata": {},
   "source": [
    "## 4. How can I load and visualize the imaging data?\n",
    "\n",
    "### Load a low-resolution overview with zarr and dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf620f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import dask.array as da\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "store = s3fs.S3Map(\n",
    "    root=f'{bucket}/{example_asset}/exaspim/image.ome.zarr/3',  # Low-res pyramid level\n",
    "    s3=fs\n",
    ")\n",
    "z = zarr.open(store, mode='r')\n",
    "data = da.from_zarr(z)\n",
    "\n",
    "print(f\"Shape (Z, Y, X): {data.shape}\")\n",
    "print(f\"Dtype: {data.dtype}\")\n",
    "print(f\"Chunk size: {data.chunks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a maximum intensity projection (MIP) of a coronal subvolume\n",
    "mid_z = data.shape[0] // 2\n",
    "subvol = data[mid_z - 25 : mid_z + 25, :, :].compute()\n",
    "mip = subvol.max(axis=0)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.imshow(mip, cmap='gray', vmin=np.percentile(mip, 1), vmax=np.percentile(mip, 99.5))\n",
    "plt.title('Maximum Intensity Projection â€” Coronal View')\n",
    "plt.colorbar(label='Fluorescence intensity')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db315fe",
   "metadata": {},
   "source": [
    "### Interactive 3D visualization with napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to launch napari viewer (requires local installation)\n",
    "# import napari\n",
    "# viewer = napari.Viewer()\n",
    "# viewer.open(\n",
    "#     f\"s3://{bucket}/{example_asset}/exaspim/image.ome.zarr\",\n",
    "#     plugin=\"napari-ome-zarr\"\n",
    "# )\n",
    "# napari.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764dc08e",
   "metadata": {},
   "source": [
    "## 5. A deeper look: exploring interhemispheric connectivity\n",
    "\n",
    "### Visualizing callosal projections\n",
    "\n",
    "The corpus callosum is the primary white matter tract connecting the two cortical hemispheres. In this dataset, labeled projection neurons send their axons through the corpus callosum to innervate the contralateral hemisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e834b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load a subvolume centered on the corpus callosum\n",
    "# Define region of interest coordinates based on atlas registration\n",
    "# or manual annotation of the midline crossing point.\n",
    "\n",
    "# cc_z_range = slice(z_start, z_end)\n",
    "# cc_y_range = slice(y_start, y_end)\n",
    "# cc_x_range = slice(x_start, x_end)\n",
    "# cc_roi = data[cc_z_range, cc_y_range, cc_x_range].compute()\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.imshow(cc_roi.max(axis=0), cmap='magma')\n",
    "# plt.title('Corpus Callosum Region â€” Axonal Crossing')\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537d2b4",
   "metadata": {},
   "source": [
    "### Quantifying fluorescence across hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd20d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute a fluorescence intensity profile across the midline\n",
    "# to reveal the spatial distribution of callosal projections.\n",
    "\n",
    "# profile = np.mean(mip, axis=0)  # Mean along dorsal-ventral axis\n",
    "# x_coords = np.arange(len(profile))\n",
    "# midline = len(profile) // 2\n",
    "\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.plot(x_coords, profile, 'k-', linewidth=0.5)\n",
    "# plt.axvline(x=midline, color='red', linestyle='--', alpha=0.7, label='Midline')\n",
    "# plt.fill_between(x_coords[:midline], profile[:midline], alpha=0.3, color='blue', label='Left hemisphere')\n",
    "# plt.fill_between(x_coords[midline:], profile[midline:], alpha=0.3, color='green', label='Right hemisphere')\n",
    "# plt.xlabel('Medial-Lateral position (pixels)')\n",
    "# plt.ylabel('Mean fluorescence intensity')\n",
    "# plt.title('Interhemispheric Fluorescence Distribution')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7ac174",
   "metadata": {},
   "source": [
    "## 6. Community Challenge ðŸ§ \n",
    "\n",
    "**Can you develop an automated method to detect, segment, and trace individual callosal axons crossing the corpus callosum in whole-brain ExA-SPIM data?**\n",
    "\n",
    "Specifically:\n",
    "\n",
    "1. **Detection:** Identify the location of labeled cell bodies in the cortex and their primary axons\n",
    "2. **Tracing:** Follow individual axons through the corpus callosum to their contralateral targets\n",
    "3. **Classification:** Characterize projection patterns â€” do axons from a given cortical area project to homotopic or heterotopic contralateral regions?\n",
    "\n",
    "This challenge is relevant to understanding how cortical areas communicate across hemispheres and could leverage modern deep learning approaches for neuron tracing (e.g., flood-filling networks, transformer-based segmentation).\n",
    "\n",
    "We welcome solutions, analyses, and derived datasets. Please contact **florent.haiss@pasteur.fr** if you would like to discuss your approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44594338",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "```\n",
    "pip install boto3 s3fs zarr dask numpy matplotlib napari napari-ome-zarr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce4c6b9",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [OME-Zarr specification (OME-NGFF)](https://ngff.openmicroscopy.org/)\n",
    "- [ExA-SPIM: Expansion-assisted selective plane illumination microscopy](https://pmc.ncbi.nlm.nih.gov/articles/PMC12208669/)\n",
    "- [Allen Institute for Neural Dynamics â€” Data Access](https://allenneuraldynamics.github.io/data.html)\n",
    "- [aind-data-schema](https://github.com/AllenNeuralDynamics/aind-data-schema/)\n",
    "- [Registry of Open Data on AWS](https://registry.opendata.aws/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
